{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4so2lNx3mmUe3h9ZGYL7j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Balavignesh-25/Resume_Parser/blob/main/Secure_AI_Resume_Job_Matcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update package lists\n",
        "!apt-get update -qq\n",
        "# Install Tesseract OCR and Poppler utilities for PDF text extraction\n",
        "!apt-get install -y tesseract-ocr poppler-utils\n",
        "# Install Python libraries: pytesseract for OCR, pillow for image processing, pdf2image for PDF to image conversion, groq for LLM, and gradio for UI\n",
        "!pip install -q pytesseract pillow pdf2image groq gradio\n",
        "# Install PyPDF2 for direct PDF text extraction\n",
        "!pip install -q PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEw7WU8ymSvd",
        "outputId": "65bc00f9-6951-4141-ff48-a67eebce542a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qXBCK4I9k5xP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import http.client\n",
        "import hashlib\n",
        "import pytesseract\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go # Although imported, plotly is not used in the final Gradio app logic.\n",
        "import gradio as gr\n",
        "\n",
        "from groq import Groq\n",
        "from datetime import datetime # Although imported, datetime is not used in the final Gradio app logic.\n",
        "from pdf2image import convert_from_path\n",
        "import PyPDF2\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the code is running in Google Colab\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False"
      ],
      "metadata": {
        "id": "kmLxLOuHmvAr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_secret(key_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Securely fetch secrets from Colab userdata or environment variables.\n",
        "    \"\"\"\n",
        "    if IN_COLAB:\n",
        "        return userdata.get(key_name)\n",
        "    return os.getenv(key_name)\n",
        "\n",
        "# Load API keys for RapidAPI (JSearch) and Groq\n",
        "RAPIDAPI_KEY = get_secret(\"RAPIDAPI_KEY\")\n",
        "GROQ_API_KEY = get_secret(\"GROQ_API_KEY\")\n",
        "\n",
        "# Validate if API keys are loaded successfully\n",
        "if not RAPIDAPI_KEY or not GROQ_API_KEY:\n",
        "    raise EnvironmentError(\n",
        "        \"‚ùå API Keys missing!\\n\"\n",
        "        \"‚Ä¢ In Colab ‚Üí Add them in Secrets and restart runtime\\n\"\n",
        "        \"‚Ä¢ Locally ‚Üí Export them as environment variables\"\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ API keys loaded successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoUp0bBCnXEI",
        "outputId": "f9916b69-01e3-4b3f-a805-1e5969c7e33f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API keys loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English language model for spaCy. Although imported, spacy is not explicitly used for skill extraction in the current logic.\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "aRRRiuAsnZh0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping for skill variants to canonical skill names\n",
        "SKILL_MAP = {\n",
        "    \"ml\": \"machine learning\",\n",
        "    \"ai\": \"machine learning\",\n",
        "    \"sklearn\": \"machine learning\",\n",
        "    \"dl\": \"deep learning\",\n",
        "    \"nlp\": \"nlp\",\n",
        "    \"reactjs\": \"react\",\n",
        "    \"nodejs\": \"node\",\n",
        "    \"postgresql\": \"sql\",\n",
        "    \"mysql\": \"sql\",\n",
        "    \"ci/cd\": \"devops\",\n",
        "}\n",
        "\n",
        "# Define a set of canonical skills to look for in resumes\n",
        "CANONICAL_SKILLS = set(SKILL_MAP.values()) | {\n",
        "    \"python\", \"java\", \"c++\", \"sql\", \"aws\", \"docker\", \"kubernetes\",\n",
        "    \"deep learning\", \"machine learning\", \"nlp\", \"pandas\", \"numpy\",\n",
        "    \"tensorflow\", \"pytorch\", \"fastapi\", \"flask\", \"gradio\"\n",
        "}"
      ],
      "metadata": {
        "id": "kdXFKwJqnhIo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file. Tries direct text extraction first, then OCR if insufficient text is found.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        # Attempt direct text extraction using PyPDF2\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for page in reader.pages:\n",
        "                text += (page.extract_text() or \"\")\n",
        "    except:\n",
        "        pass # Ignore errors during PyPDF2 extraction\n",
        "\n",
        "    # If less than 200 characters are extracted, use OCR (Tesseract) on images of the PDF pages\n",
        "    if len(text.strip()) < 200:\n",
        "        images = convert_from_path(pdf_path)\n",
        "        for img in images:\n",
        "            text += pytesseract.image_to_string(img)\n",
        "\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "9XKz_M1inkBC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_skills(text):\n",
        "    \"\"\"\n",
        "    Extracts skills from the given text based on pre-defined canonical skills and their variants.\n",
        "    Returns a dictionary of found skills and their counts.\n",
        "    \"\"\"\n",
        "    found = {}\n",
        "\n",
        "    # Find canonical skills directly\n",
        "    for skill in CANONICAL_SKILLS:\n",
        "        matches = len(re.findall(rf\"\\b{skill}\\b\", text))\n",
        "        if matches:\n",
        "            found[skill] = matches\n",
        "\n",
        "    # Find skill variants and map them to canonical skills\n",
        "    for variant, canonical in SKILL_MAP.items():\n",
        "        matches = len(re.findall(rf\"\\b{variant}\\b\", text))\n",
        "        if matches:\n",
        "            found[canonical] = found.get(canonical, 0) + matches\n",
        "\n",
        "    return found"
      ],
      "metadata": {
        "id": "hNyYOGxlnnnj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache for job search results to avoid redundant API calls\n",
        "JOB_CACHE = {}\n",
        "\n",
        "def fetch_jobs(query, num_jobs):\n",
        "    \"\"\"\n",
        "    Fetches job listings from the JSearch API based on a query.\n",
        "    Caches results to improve performance for repeated queries.\n",
        "    \"\"\"\n",
        "    key = hashlib.md5(query.encode()).hexdigest()\n",
        "    if key in JOB_CACHE:\n",
        "        return JOB_CACHE[key][:num_jobs]\n",
        "\n",
        "    conn = http.client.HTTPSConnection(\"jsearch.p.rapidapi.com\")\n",
        "    headers = {\n",
        "        \"x-rapidapi-key\": RAPIDAPI_KEY,\n",
        "        \"x-rapidapi-host\": \"jsearch.p.rapidapi.com\",\n",
        "    }\n",
        "\n",
        "    # Construct the API endpoint for job search\n",
        "    endpoint = f\"/search?query={query.replace(' ', '%20')}&page=1&num_pages=1\"\n",
        "    conn.request(\"GET\", endpoint, headers=headers)\n",
        "\n",
        "    # Parse the API response\n",
        "    res = json.loads(conn.getresponse().read())\n",
        "    jobs = res.get(\"data\", [])\n",
        "    JOB_CACHE[key] = jobs\n",
        "    return jobs[:num_jobs]"
      ],
      "metadata": {
        "id": "wyuotRwanrTy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt for the LLM, instructing it to act as an ATS scoring engine and return only valid JSON\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an ATS scoring engine.\n",
        "Return ONLY valid JSON.\n",
        "\"\"\"\n",
        "\n",
        "def analyze_with_llm(resume, jobs):\n",
        "    \"\"\"\n",
        "    Analyzes a resume against a list of jobs using the Groq LLM to generate match scores and recommendations.\n",
        "    Includes a fallback mechanism if the LLM call fails.\n",
        "    \"\"\"\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "    try:\n",
        "        # Make a request to the Groq LLM\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\", # Specify the LLM model to use\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"Resume:\\n{resume}\\nJobs:\\n{jobs}\"}\n",
        "            ],\n",
        "            temperature=0.2, # Set a low temperature for more deterministic output\n",
        "            max_tokens=3000 # Limit the response size\n",
        "        )\n",
        "\n",
        "        # Parse the JSON response from the LLM\n",
        "        return json.loads(response.choices[0].message.content)\n",
        "\n",
        "    except:\n",
        "        # üî• Fallback mode: If LLM call fails, provide generic scores and recommendations\n",
        "        return [{\n",
        "            \"rank\": i + 1,\n",
        "            \"job_title\": job.get(\"job_title\", \"\"),\n",
        "            \"company\": job.get(\"employer_name\", \"\"),\n",
        "            \"match_score\": 50, # Default score in fallback mode\n",
        "            \"matching_skills\": [],\n",
        "            \"missing_skills\": [],\n",
        "            \"reason\": \"LLM unavailable ‚Äì fallback scoring\",\n",
        "            \"recommendation\": \"Consider\"\n",
        "        } for i, job in enumerate(jobs)]"
      ],
      "metadata": {
        "id": "gh_alN-HnxHf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_resume(pdf, query, num_jobs):\n",
        "    \"\"\"\n",
        "    Main function to process a resume PDF, extract skills, fetch jobs, and analyze matches with an LLM.\n",
        "    Returns a pandas DataFrame of job matches and a JSON string of extracted skills.\n",
        "    \"\"\"\n",
        "    # Extract text from the uploaded PDF resume\n",
        "    text = extract_text_from_pdf(pdf.name)\n",
        "    # Extract skills from the resume text\n",
        "    skills = extract_skills(text)\n",
        "\n",
        "    # Fetch job listings based on the user's query\n",
        "    jobs = fetch_jobs(query, int(num_jobs))\n",
        "    # Analyze the resume against the fetched jobs using the LLM\n",
        "    results = analyze_with_llm(text[:5000], jobs) # Pass only the first 5000 characters of the resume to the LLM\n",
        "\n",
        "    # Convert the LLM results into a pandas DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "    # Return the DataFrame and a JSON string of the extracted skills\n",
        "    return df, json.dumps(skills, indent=2)"
      ],
      "metadata": {
        "id": "NUaqYQBQn0qf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the Gradio interface for the Resume Job Matcher application\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üöÄ Secure AI Resume Job Matcher\")\n",
        "\n",
        "    # Input components\n",
        "    pdf = gr.File(label=\"Upload Resume PDF\") # Allows users to upload a PDF file\n",
        "    query = gr.Textbox(value=\"machine learning engineer\", label=\"Job Search Query\") # Text input for job search\n",
        "    num_jobs = gr.Slider(1, 10, 5, label=\"Number of Jobs to Fetch\") # Slider to select number of jobs\n",
        "\n",
        "    # Button to trigger the analysis\n",
        "    btn = gr.Button(\"Analyze\")\n",
        "\n",
        "    # Output components\n",
        "    table = gr.Dataframe(label=\"Job Match Results\") # Displays job match results in a table\n",
        "    skills = gr.Code(label=\"Extracted Skills (with confidence)\", language=\"json\") # Displays extracted skills in JSON format\n",
        "\n",
        "    # Define the interaction: when the button is clicked, call process_resume and update outputs\n",
        "    btn.click(process_resume, [pdf, query, num_jobs], [table, skills])\n",
        "\n",
        "# Launch the Gradio application\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "87Lxg2p_n7ne",
        "outputId": "17b02c93-1d04-4534-8d0f-d2c9e925ac5f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://76d73c8f394ef02fe5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://76d73c8f394ef02fe5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}