{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdfhdn6ECJ16hd92SYQ25a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Balavignesh-25/Resume_Parser/blob/main/RESUME_PARSER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -y # Update package lists\n",
        "!apt-get install -y poppler-utils tesseract-ocr # Install Poppler utilities (for PDF rendering) and Tesseract OCR engine\n",
        "\n",
        "!pip install -q pdf2image pytesseract PyPDF2 spacy groq # Install Python libraries: pdf2image, pytesseract, PyPDF2, spacy, groq\n",
        "!python -m spacy download en_core_web_sm # Download the small English language model for spaCy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PyjamRrrex4",
        "outputId": "005242b2-e2fa-4004-8465-118cea977c1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connected to cloud.r-pr\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Fetched 255 kB in 1s (198 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lPLzfRQkjLzf"
      },
      "outputs": [],
      "source": [
        "# Check if the code is running in Google Colab environment\n",
        "try:\n",
        "    from google.colab import userdata # Attempt to import userdata from google.colab\n",
        "    IN_COLAB = True # If successful, set IN_COLAB flag to True\n",
        "except ImportError:\n",
        "    IN_COLAB = False # If import fails, set IN_COLAB flag to False (e.g., running locally)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os # Import the os module for environment variable access\n",
        "\n",
        "def get_secret(key_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Securely fetch secrets from Colab userdata or environment variables.\n",
        "    Prioritizes Colab userdata if running in Colab, otherwise checks environment variables.\n",
        "    \"\"\"\n",
        "    if IN_COLAB:\n",
        "        return userdata.get(key_name) # Fetch secret from Colab userdata\n",
        "    return os.getenv(key_name) # Fetch secret from environment variables\n",
        "\n",
        "# Load API keys for RapidAPI (JSearch) and Groq services\n",
        "RAPIDAPI_KEY = get_secret(\"RAPIDAPI_KEY\") # Get RapidAPI key\n",
        "GROQ_API_KEY = get_secret(\"GROQ_API_KEY\") # Get Groq API key\n",
        "\n",
        "# Validate if API keys are loaded successfully\n",
        "if not RAPIDAPI_KEY or not GROQ_API_KEY:\n",
        "    raise EnvironmentError(\n",
        "        \"‚ùå API Keys missing!\\n\"\n",
        "        \"‚Ä¢ In Colab ‚Üí Add them in Secrets and restart runtime\\n\"\n",
        "        \"‚Ä¢ Locally ‚Üí Export them as environment variables\"\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ API keys loaded successfully\") # Confirmation message if keys are loaded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvlJGH0PlOeV",
        "outputId": "2a698a76-c2cd-4cf4-bf18-e9d0c6a7d6bf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API keys loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# OCR + SKILL EXTRACTION + JOB MATCHING\n",
        "# =============================================================================\n",
        "\n",
        "# Import necessary modules and libraries\n",
        "import os # For operating system interaction\n",
        "import re # For regular expressions\n",
        "import json # For JSON parsing and serialization\n",
        "import http.client # For making HTTP connections\n",
        "from google.colab import files # For file upload functionality in Google Colab\n",
        "from pdf2image import convert_from_path # For converting PDF pages to images\n",
        "import pytesseract # For performing Optical Character Recognition (OCR)\n",
        "import PyPDF2 # For extracting text from PDF documents\n",
        "import spacy # For Natural Language Processing (NLP) tasks\n",
        "from groq import Groq # For interacting with the Groq AI API"
      ],
      "metadata": {
        "id": "igCd3Ojxrztr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# GUARDRAIL CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Maximum allowed size for uploaded PDF resumes in megabytes\n",
        "MAX_PDF_MB = 5\n",
        "# Maximum number of characters to process from the extracted resume text\n",
        "MAX_RESUME_CHARS = 8000\n",
        "# Maximum number of skills to extract from the resume\n",
        "MAX_SKILLS = 50\n",
        "# Maximum number of job listings to fetch and analyze\n",
        "MAX_JOBS = 5"
      ],
      "metadata": {
        "id": "HtCdeGoEr2Xz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# NLP SETUP\n",
        "# =============================================================================\n",
        "\n",
        "try:\n",
        "    # Attempt to load the pre-trained small English spaCy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    # If the model is not found, raise a RuntimeError with instructions to download it\n",
        "    raise RuntimeError(\"‚ùå spaCy model missing. Run: python -m spacy download en_core_web_sm\")\n",
        "\n",
        "# =============================================================================\n",
        "# SKILL NORMALIZATION MAPPINGS\n",
        "# =============================================================================\n",
        "\n",
        "# Dictionary to map skill variants to their canonical forms (e.g., 'mysql' to 'sql')\n",
        "SKILL_NORMALIZATION = {\n",
        "    \"mysql\": \"sql\", \"postgresql\": \"sql\", \"sqlite\": \"sql\", \"database\": \"sql\",\n",
        "    \"reactjs\": \"react\", \"nodejs\": \"node\",\n",
        "    \"ml\": \"machine learning\", \"ai\": \"machine learning\",\n",
        "    \"nlp\": \"nlp\", \"deep learning\": \"deep learning\",\n",
        "    \"aws\": \"aws\", \"ec2\": \"aws\", \"s3\": \"aws\", \"lambda\": \"aws\"\n",
        "}\n",
        "\n",
        "# Set of canonical (standardized) skills to look for directly in the resume\n",
        "CANONICAL_SKILLS = {\n",
        "    \"python\",\"java\",\"c\",\"c++\",\"sql\",\"react\",\"node\",\"aws\",\n",
        "    \"machine learning\",\"deep learning\",\"nlp\",\n",
        "    \"docker\",\"kubernetes\",\"git\",\"linux\"\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# FILE VALIDATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def validate_pdf(filename, file_bytes):\n",
        "    \"\"\"\n",
        "    Validates an uploaded PDF file based on its extension and size.\n",
        "    \"\"\"\n",
        "    # Check if the file has a .pdf extension (case-insensitive)\n",
        "    if not filename.lower().endswith(\".pdf\"):\n",
        "        raise ValueError(\"‚ùå Only PDF files are allowed\")\n",
        "\n",
        "    # Calculate file size in megabytes\n",
        "    size_mb = len(file_bytes) / (1024 * 1024)\n",
        "    # Check if the file size exceeds the maximum allowed limit\n",
        "    if size_mb > MAX_PDF_MB:\n",
        "        raise ValueError(f\"‚ùå File too large ({size_mb:.2f} MB). Max {MAX_PDF_MB} MB\")"
      ],
      "metadata": {
        "id": "_Gx9aIbYr5bk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# OCR + TEXT EXTRACTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file. It first tries PyPDF2 for text extraction.\n",
        "    If that yields insufficient text, it falls back to OCR using pdf2image and pytesseract.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    # Attempt to extract text directly from the PDF using PyPDF2\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            # Iterate through each page and extract text\n",
        "            for page in reader.pages:\n",
        "                if page.extract_text():\n",
        "                    text += page.extract_text()\n",
        "    except:\n",
        "        # Ignore errors during PyPDF2 extraction and proceed to OCR if needed\n",
        "        pass\n",
        "\n",
        "    # If insufficient text was extracted, use OCR (Optical Character Recognition)\n",
        "    if len(text.strip()) < 100:\n",
        "        # Convert PDF pages to images\n",
        "        images = convert_from_path(pdf_path)\n",
        "        # Use Tesseract OCR to extract text from each image\n",
        "        for img in images:\n",
        "            text += pytesseract.image_to_string(img)\n",
        "\n",
        "    # Return the extracted text, truncated to a maximum character limit\n",
        "    return text.lower()[:MAX_RESUME_CHARS]\n",
        "\n",
        "# =============================================================================\n",
        "# SKILL EXTRACTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def extract_skills(text):\n",
        "    \"\"\"\n",
        "    Extracts skills from a given text by checking against canonical skills,\n",
        "    normalizing skill variants, and using spaCy for noun chunk extraction.\n",
        "    \"\"\"\n",
        "    skills = set()\n",
        "\n",
        "    # Check for direct matches of canonical skills in the text\n",
        "    for skill in CANONICAL_SKILLS:\n",
        "        if re.search(rf\"\\b{skill}\\b\", text):\n",
        "            skills.add(skill)\n",
        "\n",
        "    # Check for skill variants and add their canonical forms\n",
        "    for variant, canonical in SKILL_NORMALIZATION.items():\n",
        "        if re.search(rf\"\\b{variant}\\b\", text):\n",
        "            skills.add(canonical)\n",
        "\n",
        "    # Use spaCy's NLP model to find noun chunks and normalize them into skills\n",
        "    doc = nlp(text)\n",
        "    for chunk in doc.noun_chunks:\n",
        "        c = chunk.text.lower()\n",
        "        if c in SKILL_NORMALIZATION:\n",
        "            skills.add(SKILL_NORMALIZATION[c])\n",
        "\n",
        "    # Return a sorted list of unique skills, truncated to a maximum limit\n",
        "    return sorted(list(skills))[:MAX_SKILLS]\n",
        "\n",
        "# =============================================================================\n",
        "# JOB FETCHING (JSEARCH API) FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def fetch_jobs(query):\n",
        "    \"\"\"\n",
        "    Fetches job listings from the JSearch RapidAPI based on a given query.\n",
        "    \"\"\"\n",
        "    conn = http.client.HTTPSConnection(\"jsearch.p.rapidapi.com\")\n",
        "\n",
        "    # Set API key and host in headers for authentication\n",
        "    headers = {\n",
        "        \"x-rapidapi-key\": RAPIDAPI_KEY,\n",
        "        \"x-rapidapi-host\": \"jsearch.p.rapidapi.com\"\n",
        "    }\n",
        "\n",
        "    # Construct the API endpoint with the URL-encoded query\n",
        "    endpoint = f\"/search?query={query.replace(' ','%20')}&page=1&num_pages=1\"\n",
        "    conn.request(\"GET\", endpoint, headers=headers)\n",
        "\n",
        "    # Get the response, read and decode it, then parse as JSON\n",
        "    res = conn.getresponse()\n",
        "    data = json.loads(res.read().decode(\"utf-8\"))\n",
        "    conn.close()\n",
        "\n",
        "    # Check if the 'data' key exists in the response\n",
        "    if \"data\" not in data:\n",
        "        return []\n",
        "\n",
        "    # Return the top N jobs (MAX_JOBS) from the response\n",
        "    return data[\"data\"][:MAX_JOBS]\n"
      ],
      "metadata": {
        "id": "A3jZtGuWsKM-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import http.client\n",
        "from google.colab import files # Re-adding this import\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import PyPDF2\n",
        "import spacy\n",
        "from groq import Groq\n",
        "\n",
        "# =============================================================================\n",
        "# JOB MATCHING VIA GROQ (AI) FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an ATS-grade resume matcher.\n",
        "Rules:\n",
        "- Do NOT hallucinate skills\n",
        "- Match only from resume\n",
        "- Return valid JSON only\n",
        "\"\"\"\n",
        "\n",
        "def analyze_resume(resume, skills, jobs):\n",
        "    \"\"\"\n",
        "    Analyzes a resume against a list of job descriptions using the Groq API.\n",
        "    It generates a match score, matching skills, missing skills, reason, and recommendation for each job.\n",
        "    \"\"\"\n",
        "    # Initialize the Groq client with the API key\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "    job_blocks = []\n",
        "    # Format each job description into a structured block for the AI prompt\n",
        "    for i, j in enumerate(jobs, 1):\n",
        "        job_blocks.append(f\"\"\"\n",
        "Job {i}:\n",
        "Title: {j.get('job_title')}\n",
        "Company: {j.get('employer_name')}\n",
        "Description: {j.get('job_description')[:1200]} # Truncate description to prevent token overflow\n",
        "\"\"\")\n",
        "\n",
        "    # Construct the user prompt for the Groq API\n",
        "    # This includes the extracted resume skills, the full resume text, and the formatted job blocks\n",
        "    user_prompt = f\"\"\"\n",
        "Resume Skills:\n",
        "{\", \".join(skills)}\n",
        "\n",
        "Resume:\n",
        "{resume}\n",
        "\n",
        "Jobs:\n",
        "{\" \".join(job_blocks)}\n",
        "\n",
        "Return JSON:\n",
        "[\n",
        "  {{\n",
        "    \"rank\": 1,\n",
        "    \"job_title\": \"\",\n",
        "    \"company\": \"\",\n",
        "    \"match_score\": 0,\n",
        "    \"matching_skills\": [],\n",
        "    \"missing_skills\": [],\n",
        "    \"reason\": \"\",\n",
        "    \"recommendation\": \"\"\n",
        "  }}\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "    # Call the Groq API to get completion for chat messages\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\", # Specify the AI model to use\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\":SYSTEM_PROMPT}, # System message guides the AI's behavior\n",
        "            {\"role\":\"user\",\"content\":user_prompt} # User's query with resume and job details\n",
        "        ],\n",
        "        temperature=0.2, # Controls the randomness of the output (lower = more deterministic)\n",
        "        max_tokens=3000 # Maximum number of tokens in the AI's response\n",
        "    )\n",
        "\n",
        "    # Parse the AI's JSON response and return it\n",
        "    return json.loads(response.choices[0].message.content)\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN FUNCTION (Application Entry Point)\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"\\nüöÄ AI Resume Skill & Job Matcher (COLAB)\\n\")\n",
        "\n",
        "    # 1. Upload Resume PDF from the user\n",
        "    print(\"üì§ Upload resume PDF\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Handle case where no file was uploaded\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No file uploaded\")\n",
        "        return\n",
        "\n",
        "    # Get the filename and its binary content from the uploaded dictionary\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    file_bytes = uploaded[filename]\n",
        "\n",
        "    # Validate the uploaded PDF file (e.g., format, size)\n",
        "    validate_pdf(filename, file_bytes)\n",
        "\n",
        "    # Save the uploaded binary content to a temporary file locally for processing\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(file_bytes)\n",
        "\n",
        "    # 2. Extract Text content from the PDF resume\n",
        "    print(\"üìÑ Extracting text...\")\n",
        "    resume_text = extract_text_from_pdf(filename)\n",
        "\n",
        "    # 3. Extract relevant skills from the extracted resume text\n",
        "    print(\"üß† Extracting skills...\")\n",
        "    skills = extract_skills(resume_text)\n",
        "    print(f\"\\n‚úÖ Skills ({len(skills)}): {skills}\")\n",
        "\n",
        "    # 4. Prompt user for a job role query; uses 'python developer' as default\n",
        "    query = input(\"\\nüîç Job role (default: python developer): \").strip() or \"python developer\"\n",
        "\n",
        "    # 5. Fetch job listings based on the user's query from an external API\n",
        "    jobs = fetch_jobs(query)\n",
        "\n",
        "    # Handle case where no jobs were found for the given query\n",
        "    if not jobs:\n",
        "        print(\"‚ùå No jobs found\")\n",
        "        return\n",
        "\n",
        "    # 6. Use AI (Groq) to match the extracted resume skills and content with the fetched jobs\n",
        "    print(\"\\nü§ñ Matching resume with jobs...\")\n",
        "    results = analyze_resume(resume_text, skills, jobs)\n",
        "\n",
        "    # 7. Display the ranked job matching results to the user\n",
        "    print(\"\\nüéØ RANKED RESULTS\\n\")\n",
        "    for r in results:\n",
        "        print(\"=\"*80)\n",
        "        print(f\"#{r['rank']} | {r['job_title']} @ {r['company']}\")\n",
        "        print(f\"Score: {r['match_score']}% | {r['recommendation']}\")\n",
        "        print(f\"Matched: {r['matching_skills']}\")\n",
        "        print(f\"Missing: {r['missing_skills']}\")\n",
        "        print(f\"Reason: {r['reason']}\")\n",
        "\n",
        "    # 8. Save the detailed job matching results to a JSON file\n",
        "    with open(\"job_match_results.json\",\"w\") as f:\n",
        "        json.dump(results,f,indent=2)\n",
        "    print(\"\\nüíæ Results saved to job_match_results.json\")\n",
        "    print(\"\\nüéâ DONE\")\n",
        "\n",
        "# =============================================================================\n",
        "# RUN APPLICATION\n",
        "# =============================================================================\n",
        "\n",
        "# Entry point for executing the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "Bp--6OHcjrD1",
        "outputId": "0b7650e8-424f-4256-e08c-774f156a0caa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ AI Resume Skill & Job Matcher (COLAB)\n",
            "\n",
            "üì§ Upload resume PDF\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae0ca656-ccb1-4b98-a83b-3d4e75d0ff42\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae0ca656-ccb1-4b98-a83b-3d4e75d0ff42\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Bala Vignesh Resume - AI (Integra connect) .pdf to Bala Vignesh Resume - AI (Integra connect) .pdf\n",
            "üìÑ Extracting text...\n",
            "üß† Extracting skills...\n",
            "\n",
            "‚úÖ Skills (7): ['c', 'c++', 'deep learning', 'java', 'machine learning', 'python', 'sql']\n",
            "\n",
            "üîç Job role (default: python developer): Machine Learning Intern\n",
            "\n",
            "ü§ñ Matching resume with jobs...\n",
            "\n",
            "üéØ RANKED RESULTS\n",
            "\n",
            "================================================================================\n",
            "#1 | Summer 2026 Artificial Intelligence/Machine Learning Associate Internship @ General Dynamics\n",
            "Score: 80% | Consider highlighting any data analytics or database experience in the resume, even if it's not explicitly mentioned.\n",
            "Matched: ['machine learning', 'python']\n",
            "Missing: ['data analytics', 'database']\n",
            "Reason: The job description mentions AI and machine learning, which are skills mentioned in the resume. However, it requires data analytics and database skills, which are not explicitly mentioned.\n",
            "================================================================================\n",
            "#2 | AI Applied Research & Machine Learning Internship (PhD) @ GEICO\n",
            "Score: 70% | Consider applying for other internships that do not require a PhD, or highlight any research experience in AI and machine learning.\n",
            "Matched: ['machine learning', 'python']\n",
            "Missing: ['PhD in AI research', 'machine learning engineering']\n",
            "Reason: The job description requires a PhD in AI research and machine learning engineering, which is not mentioned in the resume.\n",
            "================================================================================\n",
            "#3 | Student Intern - Analytics & Machine Learning @ WSSC Water\n",
            "Score: 85% | Consider highlighting any database experience in the resume, even if it's not explicitly mentioned.\n",
            "Matched: ['machine learning', 'python', 'data analysis']\n",
            "Missing: ['database']\n",
            "Reason: The job description mentions analytics and machine learning, which are skills mentioned in the resume. However, it requires database skills, which are not explicitly mentioned.\n",
            "================================================================================\n",
            "#4 | Data Science and Machine Learning Intern @ MITRE\n",
            "Score: 75% | Consider highlighting any data science experience in the resume, even if it's not explicitly mentioned.\n",
            "Matched: ['machine learning', 'python', 'data analysis']\n",
            "Missing: ['data science']\n",
            "Reason: The job description mentions data science, which is not explicitly mentioned in the resume.\n",
            "================================================================================\n",
            "#5 | 2026 Machine Learning Engineering Undergrad Intern @ The Aerospace Corporation\n",
            "Score: 80% | Consider highlighting any machine learning engineering experience in the resume, even if it's not explicitly mentioned.\n",
            "Matched: ['machine learning', 'python']\n",
            "Missing: ['machine learning engineering']\n",
            "Reason: The job description requires machine learning engineering skills, which are not explicitly mentioned in the resume.\n",
            "\n",
            "üíæ Results saved to job_match_results.json\n",
            "\n",
            "üéâ DONE\n"
          ]
        }
      ]
    }
  ]
}